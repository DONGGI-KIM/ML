{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter 02.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "OF9KyMDDwtQQ"
      ],
      "authorship_tag": "ABX9TyPESNkYJRMlu0dVJDM7xvz7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DONGGI-KIM/ML/blob/main/chapter_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD8b0kMia__8"
      },
      "source": [
        "## sklearn study"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHwzSVAcaQP7"
      },
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqLa0v7PitGU"
      },
      "source": [
        "### first ml - iris classification by decision tree\n",
        "* feature = sepal length, width, petal length, width "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHnRBJtcinl9"
      },
      "source": [
        "from sklearn.datasets import load_iris #sklearn에 존재하는 학습 dataset이 있다.\n",
        "from sklearn.tree import DecisionTreeClassifier #sklearn에 존재하는 tree 기반 ml중 결정트리 class를 사용할 것이다.\n",
        "from sklearn.model_selection import train_test_split #학습 데이터와 검증 데이터, 예측 데이터로 데이터를 분리하거나 최적의 하이퍼 파라미터로 평가"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZwYUaMukHWv"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# iris data loading\n",
        "iris = load_iris()\n",
        "iris_data = iris.data #주어진 데이터의 feature 값들을 확인하였다.\n",
        "iris_label = iris.target #데이터 label을 확인할 수 있다. 0,1,2로 표시 _names 붙이면 실제값 확인가능\n",
        "\n",
        "print(type(iris))\n",
        "print(iris) #보면 dict 형태로 분리되어있다. key등을 활용해서 내가 dataframe을 만들어야 함.\n",
        "#확인해 보자.. 둘다 numpy형태로 가지고 있다.\n",
        "print('iris target 값: ', iris_label)\n",
        "print('iris target 명칭: ', iris.target_names)\n",
        "print('iris data feature 명칭', iris.feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC__jL9TlnMY"
      },
      "source": [
        "#dataframe 변환\n",
        "iris_df = pd.DataFrame(iris) #안된다 dict형태라서 내가 바꿔주자. 음 dict변환 바로 사용해도 될듯?\n",
        "print(iris.shape)\n",
        "print(iris.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6ci2Wcqmbr0"
      },
      "source": [
        "#dataframe 변환\n",
        "iris_df = pd.DataFrame(data = iris.data, columns = iris.feature_names) #iris.data는 data만 있어서 column 명정해야함\n",
        "iris_df['label'] = iris.target #없던 새로운 column 추가\n",
        "iris_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw4bQ1fxoP68"
      },
      "source": [
        "#평가를 위해 테스트 데이터 세트와 트레이닝 데이터 세트 분리가 필요하다.\n",
        "X_train,X_test,y_train,y_test = train_test_split(iris_data,iris_label,test_size = 0.2, random_state = 11)\n",
        "# feature도 test train 나누고 label(result) 도 같이 test train 나눠야 한다. \n",
        "#하나에 두개씩 필요 iris_data에 대해서 2개 X, label에 대해서 2개\n",
        "# size = 0.2 test가 0.2 비율이다.\n",
        "# random_state = seed 고정된 추출을 위해서 # 계속 랜덤 생성이라면 안쓰면 된다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxZ1ekYEuQ2E"
      },
      "source": [
        "df_clf = DecisionTreeClassifier(random_state =11) \n",
        "#객체 생성 #random_state 는 고정값을 보려고 설정"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBzqwtauvDfx"
      },
      "source": [
        "#학습 수행\n",
        "df_clf.fit(X_train, y_train) # fit #train data 학습"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWyzeAV3vRPU"
      },
      "source": [
        "#결과 확인\n",
        "pred = df_clf.predict(X_test) #y를 잘 뽑아내나 확인.."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBweVvlBvjyl"
      },
      "source": [
        "#평가 #여기서는 accuracy_score(실제결과,예측결과) 를사용 #정확성만 평가\n",
        "from sklearn.metrics import accuracy_score \n",
        "print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test,pred))) #0: .f 소수점 4자리까지 표현"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF9KyMDDwtQQ"
      },
      "source": [
        "### 하나의 ml을 완성한 것이다. 프로세스를 확인해 보자\n",
        "* 데이터 세트 분리 : 목적에 맞게 train test \n",
        "* 모델 학습 : ml 돌리기 fit()\n",
        "* 예측수행 : .predict()\n",
        "* 평가 : 다양한 방법이 있지만 accuracy_score()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mti8lmiM1sdy"
      },
      "source": [
        "## 데이터세트를 살펴보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKnz7xbNwqyY"
      },
      "source": [
        "# 예제 데이터 세트 key 확인\n",
        "keys = iris.keys()\n",
        "print('iris keys :', keys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRDQujAawIHH"
      },
      "source": [
        "#data 를 디테일 하게 봐보자\n",
        "print('type : ', type(iris.feature_names))\n",
        "print('shape : ', len(iris.feature_names)) #list 라서 shape 불가\n",
        "print('type : ', type(iris.target_names)) #array 형태\n",
        "print('shape :', len(iris.target_names)) #len 됨.. 1차원이라서\n",
        "print('shape : ', iris.target_names.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzN2hgOAhEaj"
      },
      "source": [
        "## Model Selection 모듈 소개"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxMQYgWp3sG0"
      },
      "source": [
        "#분리하지 않고 진행한다면 어떻게 진행될까?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "dt_clf = DecisionTreeClassifier()\n",
        "train_data = iris.data\n",
        "train_label = iris.target\n",
        "dt_clf.fit(train_data, train_label) \n",
        "# test 없이 다 train 하고 예측을 다시 학습데이터로 하게된다..\n",
        "\n",
        "pred = df_clf.predict(train_data)\n",
        "print('예측 정확도: ', accuracy_score(train_label,pred) ) #?? 왜 1.0 이 아니지.. 멍청한가.."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXOx8v8Nkn0S"
      },
      "source": [
        "* 중요한 parameter\n",
        "* test_size : 몇 퍼센트 test로 쓸 것인가.\n",
        "* shuffle : 섞을 것인가 default = true 무조건 섞어야되서 안건들어도 됨.\n",
        "* radom_state : seed 설정.. 돌릴때 마다 동일하게 뽑기 위해서"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr4GBSA9jP17"
      },
      "source": [
        "#train 데이터와 test 데이터를 충분히 확보하는것이 중요하다.\n",
        "dt_clf = DecisionTreeClassifier()\n",
        "iris_data = load_iris()\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(iris_data.data, iris_data.target, test_size = 0.3, random_state =121)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "pred = dt_clf.predict(X_test)\n",
        "print('예측 정확도: {0:4f}'.format(accuracy_score(y_test,pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lakqm3TzoTeS"
      },
      "source": [
        "#평가를 보장해 주는 것과 학습데이터 확보가 굉장히 중요한 요인이다.\n",
        "#어떻게 해결할 수 있을까?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMFODEFJodo5"
      },
      "source": [
        "## 교차검증\n",
        "* 오버피팅 현상을 막기위해서 필요하다. 학습데이터나 테스트데이터에 과도하게 적합하게 될 수도 있기 때문\n",
        "* 모의고사를 여러번 치루는것\n",
        "* 학습데이터를 = 학습 데이터 세트일부 + 나머지 일부를 검증데이터 세트(일차 평가) 로 나눔\n",
        "* 최종 테스트 데이터"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV7QRl3LofRR"
      },
      "source": [
        "# 1 사이킷런 아이리스 데이터, 결정트리, kfold 불러오기\n",
        "# 2 데이터별 변수 지정\n",
        "# 3 kfold 반복수 설정 회차별 정확도 담을 k 설정해야함..\n",
        "# 4 for문돌려서 k 별 train test 설정\n",
        "# 5 반복시 마다 정확도를 확인해 평균을 낸다.\n",
        "# 6 책에는 없지만, 최종테스트 데이터는 어떻게 확인할까?\n",
        "from sklearn.tree import DecisionTreeClassifier #결정나무 불러오기\n",
        "from sklearn.metrics import accuracy_score      #정확도 측정\n",
        "from sklearn.model_selection import KFold       #교차검증 할거임\n",
        "\n",
        "iris = load_iris()\n",
        "features = iris.data #feature 값들만 뽑을것 input임\n",
        "labels = iris.target #output 모음집...\n",
        "dt_clf = DecisionTreeClassifier(random_state =156) #split을 따로안하고 kfold 할거라서 여기서 선언해준다.\n",
        "\n",
        "#kfold 객체 생성하고 k수넣어줌 이후 회차별 정확도담을 list 생성\n",
        "kfold = KFold(n_splits =5)\n",
        "cv_accuarcy = []\n",
        "print('붓꽃 데이터 세트 크기:', features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJXpDZZF00S2"
      },
      "source": [
        "n_iter = 0 #그냥 반복 몇번 했는지 확인할려고 근데 if 걸어서 test까지 넣어 볼만하다고 생각한다.\n",
        "\n",
        "#kfold.split = 학습데이터와 검증데이터 인덱스!!! 를 array로 변환\n",
        "for train_index, test_index in kfold.split(features) :\n",
        "  #kfold.split() 로 분리된 데이터를 넣어 줘야함.\n",
        "  X_train, X_test = features[train_index] , features[test_index] #인덱스를 넣어서 슬라이싱?\n",
        "  y_train, y_test = labels[train_index] , labels[test_index]\n",
        "  dt_clf.fit(X_train,y_train)\n",
        "  pred = df_clf.predict(X_test)\n",
        "  n_iter +=1 \n",
        "\n",
        "  #정확도 측정해서 list에 넣기\n",
        "  accuracy = np.round(accuracy_score(y_test,pred),4)\n",
        "  train_size = X_train.shape\n",
        "  test_size = X_test.shape\n",
        "  #확인 print\n",
        "  print('\\n #{} 교차 검증 정확도 : {}, 학습 데이터 크기 :{}, 검증데이터 크기{}'.format(n_iter,accuracy,train_size,test_size))\n",
        "  print(\"\\n #{} 검증세트 인덱스 {}\".format(n_iter, test_index[0:5]))\n",
        "  cv_accuarcy.append(accuracy) \n",
        "  if n_iter == 5 :\n",
        "    pred =df_clf.predict([[7.1,3.7, 1.8,0.6]])\n",
        "    print('\\n #', pred) #되네?\n",
        "\n",
        "print('\\n # 평균 검증 정확도:',np.mean(cv_accuarcy)) #마지막 총 정확도\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A48CYY3xMGPy"
      },
      "source": [
        "## Stratified K fold\n",
        "* imbalanced 분포도를 가진 레이블 데이터 집합을 다룰 때 사용\n",
        "* 데이터의 분포를 먼저 고려하고 이 분포와 동일하게 학습과 검증 데이터 세트를 분배한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5PObXjB4ABm"
      },
      "source": [
        "import pandas as pd\n",
        "iris = load_iris()\n",
        "iris_df = pd.DataFrame(data=iris.data, columns = iris.feature_names)\n",
        "iris_df['label'] = iris.target \n",
        "iris_df['label'].value_counts() #항목별 개수를 세서 보여줌 2,1,0 으로 분류되는 value가 각각 50개 씩 있다는뜻."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKEGqKGbMwJ9"
      },
      "source": [
        "#데이터 불균형을 유도해서 보여주겠음\n",
        "kfold = KFold(n_splits=3)\n",
        "n_iter = 0\n",
        "for train_index, test_index in kfold.split(iris_df) :\n",
        "  n_iter += 1\n",
        "  label_train = iris_df['label'].iloc[train_index] #강제로 순서대로 뽑아냄 train_index 처음에 0~50 이니깐 실제데이터 인덱스에서 그에 해당하는것을 뽑음\n",
        "  label_test = iris_df['label'].iloc[test_index]\n",
        "  print('## 교차 검증: {}'.format(n_iter))\n",
        "  print('학습 레이블 데이터 분포:\\n',label_train.value_counts())\n",
        "  print('검증 레이블 데이터 분포:\\n',label_test.value_counts())\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TrpLTVxQLWE"
      },
      "source": [
        "#위에는 제대로 학습이 안됨 불균형 그자체\n",
        "#배우지 않은것을 예측해야 하기 때문"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzLTOJp1Q_10"
      },
      "source": [
        "#stratifiedKFOLD 를 사용해보자. 중요한건 분포도 확인을위해 label데이터도 입력\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "skf = StratifiedKFold(n_splits=3) #데이터를 3개로 나눌것이다.\n",
        "n_iter = 0\n",
        "\n",
        "for train_index, test_index in skf.split(iris_df, iris_df['label']) : #라벨도 넣어야함.. \n",
        "  n_iter += 1\n",
        "  label_train = iris_df['label'].iloc[train_index]\n",
        "  label_test = iris_df['label'].iloc[test_index]\n",
        "  print('## 교차검증 : {}'.format(n_iter))\n",
        "  print(\"학습 레이블 데이터 분포:\\n\", label_train.value_counts())\n",
        "  print('검증 레이블 데이터 분포:\\n', label_test.value_counts())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imT8bnQGXTFH"
      },
      "source": [
        "#처음부터\n",
        "iris = load_iris()\n",
        "features = iris.data\n",
        "labels = iris.target\n",
        "dt_clf = DecisionTreeClassifier(random_state=156) # 모델 생성\n",
        "skfold = StratifiedKFold(n_splits = 3) #검증 방법 설정\n",
        "n_iter = 0\n",
        "cv_accuracy = [] #정확도 담을 것\n",
        "\n",
        "for train_index, test_index in skfold.split(features, labels) : #분포를 위해 label도 같이 넣어줘야함\n",
        "  X_train, X_test = features[train_index], features[test_index] #정해진 인덱스 번호들이 features로\n",
        "  y_train, y_test = labels[train_index], labels[test_index] #역시 labels로\n",
        "  dt_clf.fit(X_train,y_train) #features와 label에 대해 학습\n",
        "  pred = dt_clf.predict(X_test)\n",
        "\n",
        "  #반복마다 정확도 측정\n",
        "  n_iter +=1\n",
        "  accuracy = np.round(accuracy_score(y_test,pred),4)\n",
        "  train_size = X_train.shape[0]\n",
        "  test_size = X_test.shape[0]\n",
        "  print('\\n #{} 검증 세트 인덱스 : {}'.format(n_iter, test_index))\n",
        "  cv_accuarcy.append(accuracy)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDlG_3CVbmeK"
      },
      "source": [
        "* 교차 검증을 보다 간편하게 cross val score()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M93iokkHbPMy"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score, cross_validate\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris_data = load_iris()\n",
        "dt_clf = DecisionTreeClassifier(random_state=156)\n",
        "\n",
        "data = iris_data.data\n",
        "label = iris_data.target\n",
        "\n",
        "#성능 지표는 정확도, 교차 검증 세트는 3개\n",
        "scores = cross_val_score(df_clf, data, label, scoring='accuracy', cv=3)\n",
        "print('교차 검증별 정확도:', np.round(scores, 4))\n",
        "print('평균 검증 정확도:', np.round(np.mean(scores),4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKqCDXDpc03R"
      },
      "source": [
        "* GridSearchCV \n",
        "* 하이퍼 파라미터 튜닝까지 같이해줌\n",
        "* 하이퍼 파라미터 목록을 건내주면, 하나씩 해주면서 가장 좋은 결과를 뽑아줌"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5YyMkM9crih"
      },
      "source": [
        "grid_parameters = {\n",
        "    'max_depth':[1,2,3],\n",
        "    'min_samples_split' : [2,3],\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn6r35KUdsXT"
      },
      "source": [
        "* estimator : classifier, regressor, pipeline\n",
        "* param_grid : key + 리스트 값을 가지는 딕셔너리가 주어집니다. estimator의 튜닝을 위해 파라미터명과 사용될 여러 파라미터 값을 지정합니다.\n",
        "* scoring : 성능 평가 방법 지정\n",
        "* cv : 교차 검증을 위해 분할되는 학습 / 테스트 세트의 개수를 지정합니다.\n",
        "* refit : True 가 default hyper parameter 결정되면 재학습 시킨다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZPe-lr6dre4"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#데이터를 로딩하고 학습 데이터와 테스트 데이터 분리\n",
        "iris_data = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size = 0.2, random_state=121)\n",
        "\n",
        "dtree = DecisionTreeClassifier()\n",
        "\n",
        "### 파라미터를 딕셔너리 형태로 설정\n",
        "parameters = {'max_depth' : [1,2,3], 'min_samples_split':[2,3]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka-0xF7vgix5"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "grid_dtree = GridSearchCV(dtree, param_grid= parameters, cv=3 , refit =True)\n",
        "\n",
        "#붓꽃 학습 데이터로 param_grid의 하이퍼 파라미터를 순차적으로 학습/평가\n",
        "grid_dtree.fit(X_train,y_train)\n",
        "\n",
        "#GridSearchCV 결과를 추출해 DataFrame으로 변환\n",
        "score_df = pd.DataFrame(grid_dtree.cv_results_)\n",
        "score_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-yQ9P1FhZC_"
      },
      "source": [
        "print('최적 파라미터:',grid_dtree.best_params_)\n",
        "print('최고 정확도 : {0:.4f}'.format(grid_dtree.best_score_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeJlUOYeickI"
      },
      "source": [
        "* estimator를 학습해 최적 하이퍼 파라미터 튜닝을 수행한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inapV0UgiJ_R"
      },
      "source": [
        "estimator = grid_dtree.best_estimator_\n",
        "\n",
        "pred = estimator.predict(X_test)\n",
        "print('테스트 데이터 세트 정확도 : {0:.4f}'.format(accuracy_score(y_test,pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogq9hdHNi7Nc"
      },
      "source": [
        "* 레이블 인코딩 : 한글을 숫자로 바꿔줄 때, 해줘야함... \n",
        "* 단점 존재 : 선형회귀와 같은 곳에 사용하면, 큰 숫자에 가중치를 더 부여할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3C3Nx57i2fP"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "items = ['TV', '냉장고', '전자렌지','컴퓨터','선풍기','선풍기','믹서','믹서']\n",
        "\n",
        "#LabelEncoder 를 객체로 생성한 후, fit() 과 transform()으로 레이블 인코딩 수행.\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(items)\n",
        "labels = encoder.transform(items)\n",
        "print('변환값:', labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyTcQ_rKkMWX"
      },
      "source": [
        "# 해당하는 것이 무엇인지 확인하기위한 법\n",
        "print('인코딩 클래스:', encoder.classes_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww1km0eWkWwe"
      },
      "source": [
        "print('디코딩 원본값:', encoder.inverse_transform([4,5,2,0,1,1,3,3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBGIWQJKkqlH"
      },
      "source": [
        "* 원-핫 인코딩\n",
        "* 컴퓨터가 구별할 수 있게 해주는 법...\n",
        "* 단, 문자열 값이 숫자형 값으로 변환돼야 한다.\n",
        "* 게다가 입력값으로 2차원 데이터가 필요하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oKRVYq6knaW"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "items = ['TV', '냉장고', '전자렌지','컴퓨터','선풍기','선풍기','믹서','믹서']\n",
        "\n",
        "# 먼저 숫자 값으로 변환을 위해 Labelencoder로 변환합니다.\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(items)\n",
        "labels = encoder.transform(items)\n",
        "#2차원 데이터로 변환합니다.\n",
        "labels = labels.reshape(-1,1)\n",
        "\n",
        "# 원-핫 인코딩 적용\n",
        "oh_encoder = OneHotEncoder()\n",
        "oh_encoder.fit(labels)\n",
        "oh_labels = oh_encoder.transform(labels)\n",
        "print(type(oh_labels))\n",
        "print(oh_labels)\n",
        "print('#########################')\n",
        "print(oh_labels.toarray())\n",
        "print(oh_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NExLZgqWmhQg"
      },
      "source": [
        "#판다스에서 더욱 쉽게하자!!!!\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({'items':['TV', '냉장고', '전자렌지','컴퓨터','선풍기','선풍기','믹서','믹서']})\n",
        "pd.get_dummies(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNioX7_vm9Pi"
      },
      "source": [
        "* 피처 스케일링과 정규화 !!!!\n",
        "* 정규화는 최소0~최대1 변환, 평균0 이고 분산이 1인 가우시안 정규 분포를 가진 값으로 변환한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNUT1rQ5m7DW"
      },
      "source": [
        "#StandardScaler\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "#붓꽃 데이터 세트를 로딩하고 DataFrame으로 변환합니다.\n",
        "iris = load_iris()\n",
        "iris_data = iris.data\n",
        "iris_df = pd.DataFrame(data = iris_data, columns = iris.feature_names)\n",
        "\n",
        "print('feature 들의 평균 값')\n",
        "print(iris_df.mean())\n",
        "print('\\nfeature 들의 분산 값')\n",
        "print(iris_df.var())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYuOM7QtoV1Y"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 객체 생성\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(iris_df)\n",
        "iris_scaled = scaler.transform(iris_df)\n",
        "\n",
        "# ndarray 로 반환돼 이를 dataframe형식으로 변환\n",
        "iris_df_scaled = pd.DataFrame(data = iris_scaled, columns = iris.feature_names)\n",
        "print('feature 들의 평균 값')\n",
        "print(iris_df_scaled.mean())\n",
        "print('\\nfeature 들의 분산 값')\n",
        "print(iris_df_scaled.var())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lfz16beypfAI"
      },
      "source": [
        "* MinMaxScaler \n",
        "* 데이터가 가우시안 분포가 아닐때 사용하도록 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxkTtog5pbeJ"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(iris_df)\n",
        "iris_scaled = scaler.transform(iris_df)\n",
        "\n",
        "#역시 ndarray 로 반환되니까 dataframe로 반환\n",
        "iris_df_scaled = pd.DataFrame(data = iris_scaled, columns = iris.feature_names)\n",
        "iris_df_scaled.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b83Zt7Cwqasu"
      },
      "source": [
        "* 스케일링을 할때는 두번 선언 ㄴㄴ\n",
        "* 같은 기준으로 스케일링을 해야한다.\n",
        "* 또 선언하면 원하는 결과 x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALRmTV1AqMT4"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "train_array = np.arange(0,11).reshape(-1,1)\n",
        "test_array = np.arange(0,6).reshape(-1,1)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train_array)\n",
        "train_scaled = scaler.transform(train_array)\n",
        "\n",
        "print('원본 train_array 데이터 : ', np.round(train_array.reshape(-1),2))\n",
        "print('바뀐거:',np.round(train_scaled.reshape(-1),2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJW0Yum5rTEL"
      },
      "source": [
        "scaler.fit(test_array)\n",
        "\n",
        "test_scaled = scaler.transform(test_array) # 1/5 로 scale\n",
        "\n",
        "print('원본 : ',np.round(test_array.reshape(-1),2))\n",
        "print('변경 : ',np.round(test_scaled.reshape(-1),2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09yE2TvIr1t2"
      },
      "source": [
        "* 위의 결과를 보아 학습데이터로 이미 fit이 적용된 scaler 객체를 이용해 transfomr해야한다는 것을 알 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UTHU1V8ryul"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train_array)\n",
        "train_scaled = scaler.transform(train_array)\n",
        "print('원본:',np.round(train_array.reshape(-1),2))\n",
        "print('변환:', np.round(train_scaled.reshape(-1),2))\n",
        "\n",
        "test_scaled = scaler.transform(test_array)\n",
        "print('\\n원본 : ', np.round(test_array.reshape(-1),2))\n",
        "print('\\n변환 : ', np.round(test_scaled.reshape(-1),2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE8g5LS-sxol"
      },
      "source": [
        "*포인트!!\n",
        "* 가능하다면 다 스케일링 변환을 한뒤 학습과 테스트 데이터로 분리\n",
        "* 위에처럼 안된다면 꼭 미리적용한 scaler 객체 사용할 것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdybnWm_thFl"
      },
      "source": [
        "## 타이타닉 생존자 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F-lP44zspCF"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHDz5hO5ufRs"
      },
      "source": [
        "titanic_df = pd.read_csv('/content/drive/My Drive/머신러닝 연습/앤드류응 + 파이썬 머신러닝 완벽 가이드/titanic_train.csv')\n",
        "titanic_df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHMGyZg0uydc"
      },
      "source": [
        "print('\\n train data info\\n')\n",
        "print(titanic_df.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm9Id6x_u50U"
      },
      "source": [
        "#null 처리 int는 mean으로 \n",
        "# object는 N으로 표시 변경\n",
        "titanic_df['Age'].fillna(titanic_df['Age'].mean(),inplace=True)\n",
        "titanic_df['Cabin'].fillna('N',inplace=True)\n",
        "titanic_df['Embarked'].fillna('N',inplace=True)\n",
        "\n",
        "print('데이터 null 개수 : ',titanic_df.isnull().sum().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQCqQwlZvoRc"
      },
      "source": [
        "#문자열 feature들을 처리한다\n",
        "print('성 분포 : \\n',titanic_df['Sex'].value_counts())\n",
        "print('\\ncabin 분포 : \\n',titanic_df['Cabin'].value_counts())\n",
        "print('\\nembarked 분포 : \\n',titanic_df['Embarked'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14Ofimg3wB_v"
      },
      "source": [
        "#앞의 알파벳이 중요해보인다 cabin의 경우 선실등급으로 파악됨 따라서 앞자리만 보도록하겠다.\n",
        "titanic_df['Cabin'] = titanic_df['Cabin'].str[:1]\n",
        "print(titanic_df['Cabin'].head(3))\n",
        "print('\\ncabin 분포 : \\n',titanic_df['Cabin'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_Ekdnn8w2H0"
      },
      "source": [
        "#성별에 따른 생존자 수부터 확인하자\n",
        "titanic_df.groupby(['Sex','Survived'])['Survived'].count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc0vXSGtxOFQ"
      },
      "source": [
        "sns.barplot(x='Sex',y='Survived',data=titanic_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdcixAiyxcRs"
      },
      "source": [
        "#부유함으로 나눠보자 plcass ! 추가로 성별도 함께보자..\n",
        "sns.barplot(x='Pclass',y='Survived',hue='Sex',data = titanic_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUlb7qb5x3Ru"
      },
      "source": [
        "* 여자의 경우 1 2 보다 3이 낮다\n",
        "* 남자의 경우 1이 2 3보다 높다 \n",
        "* 여자를 다 구하고 남자를 구할 때 역시 부유함이 척도로 된다는 것을 알 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8EWnoZAxwcT"
      },
      "source": [
        "#age는 연속적이라 임의로 그룹을 지어서 분류하도록 하겠다.\n",
        "def get_category(age):\n",
        "  cat = ''\n",
        "  if age <= -1:\n",
        "    cat= 'Unknown'\n",
        "  elif age <= 5 :\n",
        "    cat = 'Baby'\n",
        "  elif age <= 12 :\n",
        "    cat = 'Child'\n",
        "  elif age <= 18 :\n",
        "    cat = 'Teenager'\n",
        "  elif age <=25 :\n",
        "    cat = 'Student'\n",
        "  elif age <= 35 :\n",
        "    cat = 'Young Adult'\n",
        "  elif age <= 60 :\n",
        "    cat = 'Adult'\n",
        "  else :\n",
        "    cat ='Elderly'\n",
        "\n",
        "  return cat\n",
        "\n",
        "plt.figure(figsize=(10,6)) #볼게 많으니깐 size를 키움\n",
        "group_names = ['Unknown','Baby','Child','Teenager','Student','Young Adult','Adult','Elderly']\n",
        "#순서를 정렬해줄려고 표현\n",
        "\n",
        "titanic_df['Age_cat'] = titanic_df['Age'].apply(lambda x : get_category(x))\n",
        "sns.barplot(x='Age_cat',y='Survived',hue = 'Sex',data = titanic_df,order=group_names)\n",
        "titanic_df.drop('Age_cat',axis=1,inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fcx5EmHd0Kjf"
      },
      "source": [
        "#남은것들 문자열 카테고리 피처를 숫자형으로 바꿀것임\n",
        "#labelencoder를 사용함 # randomforest를 사용할 것이라서 그런듯 싶다.\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def encode_features(dataDF):\n",
        "  features = ['Cabin', 'Sex', 'Embarked']\n",
        "  for feature in features :\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    le = le.fit(dataDF[feature])\n",
        "    dataDF[feature] = le.transform(dataDF[feature])\n",
        "\n",
        "  return dataDF\n",
        "\n",
        "titanic_df = encode_features(titanic_df)\n",
        "titanic_df.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6poR3G5T1ZHY"
      },
      "source": [
        "#최종 처리를 해봅시다. #지금까지한\n",
        "#null 처리, 포매팅, 인코딩 포함...\n",
        "\n",
        "#Null 처리함수\n",
        "def fillna(df):\n",
        "  df['Age'].fillna(df['Age'].mean(),inplace = True)\n",
        "  df['Cabin'].fillna('N',inplace=True)\n",
        "  df['Embarked'].fillna('N',inplace=True)\n",
        "  df['Fare'].fillna(0,inplace=True)\n",
        "  return df\n",
        "\n",
        "#머신러닝 알고리즘에 불필요한 속성 제거\n",
        "def drop_features(df):\n",
        "  df.drop(['PassengerId','Name','Ticket'],axis=1,inplace = True)\n",
        "  return df\n",
        "\n",
        "#레이블 인코딩 수행\n",
        "def format_features(df):\n",
        "  df['Cabin'] = df['Cabin'].str[:1]\n",
        "  features = ['Cabin','Sex','Embarked']\n",
        "  for feature in features :\n",
        "    le = LabelEncoder()\n",
        "    le = le.fit(df[feature])\n",
        "    df[feature] = le.transform(df[feature])\n",
        "  \n",
        "  return df\n",
        "\n",
        "#앞에서 설정한 데이터 전처리 함수 호출\n",
        "def transform_features(df):\n",
        "  df = fillna(df)\n",
        "  df = drop_features(df)\n",
        "  df = format_features(df)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7ktzXWm3zTW"
      },
      "source": [
        "#원본 가져오고 피처랑 레이브 추출\n",
        "titanic_df = pd.read_csv('/content/drive/My Drive/머신러닝 연습/앤드류응 + 파이썬 머신러닝 완벽 가이드/titanic_train.csv')\n",
        "y_titanic_df = titanic_df['Survived']\n",
        "X_titanic_df = titanic_df.drop('Survived',axis =1)\n",
        "\n",
        "X_titanic_df = transform_features(X_titanic_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ_DqFlE4QCY"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test,y_train, y_test = train_test_split(X_titanic_df, y_titanic_df, test_size = 0.2, random_state = 11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU5iB1Pc4nyr"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KF2K0LFOVLo"
      },
      "source": [
        "#각 알고리즘의 Classifier 클래스 생성\n",
        "dt_clf = DecisionTreeClassifier(random_state=11)\n",
        "rf_clf = RandomForestClassifier(random_state=11)\n",
        "lr_clf = LogisticRegression()\n",
        "\n",
        "# 학습 예측 평가\n",
        "dt_clf.fit(X_train, y_train)\n",
        "dt_pred = dt_clf.predict(X_test)\n",
        "print('DecisionTreeClassifier 정확도 : {0:.4f}'.format(accuracy_score(y_test, dt_pred)))\n",
        "\n",
        "# 학습 예측 평가\n",
        "rf_clf.fit(X_train, y_train)\n",
        "rf_pred = rf_clf.predict(X_test)\n",
        "print('RandomForestClassifier 정확도 : {0:.4f}'.format(accuracy_score(y_test,rf_pred)))\n",
        "\n",
        "# 학습 예측 평가\n",
        "lr_clf.fit(X_train, y_train)\n",
        "lr_pred = lr_clf.predict(X_test)\n",
        "print('LogisticRegression 정확도 : {0:.4f}'.format(accuracy_score(y_test,lr_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO8dumdNPo0T"
      },
      "source": [
        "#KFOLD 활용\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def exec_kfold(clf, folds =5):\n",
        "  kfold = KFold(n_splits = 5) # 폴드 수만큼 예측결과 저장을 위한 리스트 객체 생성\n",
        "  scores = []\n",
        "\n",
        "  #교차검증 수행\n",
        "  for iter_count, (train_index, test_index) in enumerate(kfold.split(X_titanic_df)):\n",
        "    X_train, X_test = X_titanic_df.values[train_index], X_titanic_df.values[test_index]\n",
        "    y_train, y_test = y_titanic_df.values[train_index], y_titanic_df.values[test_index]\n",
        "    # Classifier 학습, 예측, 정확도 계산\n",
        "    clf.fit(X_train, y_train)\n",
        "    predictions = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    scores.append(accuracy)\n",
        "    print(\"교차 검증 {0} 정확도 : {1:.4f}\".format(iter_count, accuracy))\n",
        "\n",
        "  #5 개 fold에서의 평균 정확도 계산.\n",
        "  mean_score = np.mean(scores)\n",
        "  print('average accuracy : {0:.4f}'.format(mean_score))\n",
        "\n",
        "# exec_kfold 호출\n",
        "exec_kfold(dt_clf,folds = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExCZRB82Ty9E"
      },
      "source": [
        "#cross_val_score 사용\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(dt_clf, X_titanic_df, y_titanic_df, cv=5)\n",
        "for iter_count, accuracy in enumerate(scores) :\n",
        "  print('교차 검증 {0} 정확도{1:.4f}'.format(iter_count,accuracy))\n",
        "\n",
        "print('평균 정확도 : {0:.4f}'.format(np.mean(scores)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYIZY6zeU6XK"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameters = {'max_depth' : [2,3,5,10], 'min_samples_split' : [2,3,5], 'min_samples_leaf' : [1,5,8]}\n",
        "grid_dclf = GridSearchCV(dt_clf, param_grid = parameters, scoring = 'accuracy', cv =5)\n",
        "grid_dclf.fit(X_train,y_train)\n",
        "\n",
        "print('GridSearchCV 최적 하이퍼 파라미터 :',grid_dclf.best_params_)\n",
        "print('GridSearchCV 최고 정확도 : {0:.4f}'.format(grid_dclf.best_score_))\n",
        "best_dclf = grid_dclf.best_estimator_\n",
        "\n",
        "#최적 하이퍼파라미터로 예측 및 평가.\n",
        "dpredictions =  best_dclf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, dpredictions)\n",
        "print('최종 정확도 : {0:.4f}'.format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnzVh_WuW4T5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}